{"cells":[{"cell_type":"markdown","metadata":{"id":"vpztso9bQfwS"},"source":["### We import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4558,"status":"ok","timestamp":1693343440007,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"M7T-LM7yQfcg","outputId":"c4f8e7ba-66a0-4af6-f2d0-18bdec2d4bad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tf_slim in /usr/local/lib/python3.10/dist-packages (1.1.0)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf_slim) (1.4.0)\n"]}],"source":["import json\n","import pickle\n","import zipfile\n","\n","!pip install tf_slim"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":878,"status":"ok","timestamp":1693343447017,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"VUBxIsMsQT8x"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"1X5iwbCRQljB"},"source":["### We unzip the original database"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":541,"status":"ok","timestamp":1693343873766,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"YVdR3CYjQm5J"},"outputs":[],"source":["local_zip = \"/content/Aquatic_Animals.zip\"\n","zip_ref = zipfile.ZipFile(local_zip, \"r\")\n","zip_ref.extractall(\"dataset_original\")\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"OpkVkjF0QpXF"},"source":["## JSON to CSV\n","### We define the path of our JSON file\n","This process must be repeated with training and test"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1693345548767,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"dlicaaq5QrRX"},"outputs":[],"source":["type_file = \"test\"\n","path = \"/content/AquaticAnimals.json\"\n","data_file = open(path)\n","data = json.load(data_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOdV3QgzOxPs"},"outputs":[],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"G6GKFfX9QuVS"},"source":["We define the structure of the source JSON and the relevant variables that we must save in the CSV"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":208,"status":"ok","timestamp":1693345553002,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"xbr0qUIEQvnX"},"outputs":[],"source":["csv_list = []\n","'''\n","Recorremos la estructura del archivo json, extrayendo las variables relevantes\n","1. Clase\n","2. Bounding box (x inicial, y inicial, x final, y final)\n","3. Ancho y alto de la imagen.\n","4. Nombre del archivo.\n","'''\n","for classification in data:\n","  width, height = classification['width'], classification['height']\n","  image = classification['image']\n","  for item in classification['tags']:\n","    name = item['name']\n","    xmin = item['pos']['x']\n","    ymin = item['pos']['y']\n","    xmax = item['pos']['x'] + item['pos']['w']\n","    ymax = item['pos']['y'] + item['pos']['h']\n","\n","    value = (image, width, height, name, xmin, ymin, xmax, ymax)\n","    csv_list.append(value)\n","\n","column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","csv_df = pd.DataFrame(csv_list, columns = column_name)\n","\n","# We convert the dataframe to CSV\n","csv_df.to_csv(\"/content/{}_labels.csv\".format(type_file))"]},{"cell_type":"markdown","metadata":{"id":"DT2jnen_Qxmo"},"source":["## CSV to TFRecord\n","We install the TensorFlow Object Detection library"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47300,"status":"ok","timestamp":1693351643237,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"T8n3nRTGQ1Fh","outputId":"5be1a025-1951-4b10-d9ff-0acf757aaf7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","/content/models\n","Note: switching to '58d19c67e1d30d905dd5c6e5092348658fed80af'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by switching back to a branch.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -c with the switch command. Example:\n","\n","  git switch -c <new-branch-name>\n","\n","Or undo this operation with:\n","\n","  git switch -\n","\n","Turn off this advice by setting config variable advice.detachedHead to false\n","\n","HEAD is now at 58d19c67e Internal change\n","Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,004 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,179 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,251 kB]\n","Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [493 kB]\n","Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [960 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [903 kB]\n","Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [990 kB]\n","Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [21.8 kB]\n","Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [37.3 kB]\n","Fetched 7,225 kB in 3s (2,802 kB/s)\n","Reading package lists... Done\n","E: Unable to locate package python-pil\n","E: Unable to locate package python-lxml\n","/content/models/research\n","object_detection/protos/input_reader.proto:5:1: warning: Import object_detection/protos/image_resizer.proto is unused.\n","2023-08-29 23:27:17.192156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-29 23:27:18.848908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import os\n","%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","%cd /content/models/\n","!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n","!apt-get update && apt-get install -y -qq protobuf-compiler python-pil python-lxml python-tk\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","!pip install -q pycocotools\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","!python object_detection/builders/model_builder_test.py"]},{"cell_type":"markdown","metadata":{"id":"e8vueBUvQ5bf"},"source":["We take the base script and modify it to our use case"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1693352859877,"user":{"displayName":"camilo diaz","userId":"14334181777145209574"},"user_tz":300},"id":"BGJu89eLQ56L","outputId":"5c0acdd4-7597-46c2-dae4-35634f9f9893"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully created the TFRecords: /content/models/research/test.record\n"]}],"source":["from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","\n","import os\n","import io\n","import pandas as pd\n","import tensorflow as tf\n","import sys\n","sys.path.append(\"../../models/research\")\n","\n","from PIL import Image\n","from object_detection.utils import dataset_util\n","from collections import namedtuple, OrderedDict\n","\n","\n","# It is essential to replace the classes with the same classes as the project.\n","# The names must be spelled exactly the same.\n","# If there are more classes in your project you would add an ELIF.\n","def class_text_to_int(row_label):\n","    if row_label == \"Blacktip shark\":\n","      return 1\n","    elif row_label == \"Bull shark\":\n","      return 2\n","    elif row_label == \"Hammerhead shark\":\n","      return 3\n","    elif row_label == \"Lemon shark\":\n","      return 4\n","    elif row_label == \"Nurse shark\":\n","      return 5\n","    elif row_label == \"Tiger shark\":\n","      return 6\n","    elif row_label == \"Whitetip shark\":\n","      return 7\n","    elif row_label == \"Caribbean giant manta ray\":\n","      return 8\n","    elif row_label == \"Chilean devil ray\":\n","      return 9\n","    elif row_label == \"Spotted eagle ray\":\n","      return 10\n","    elif row_label == \"Ocellated eagle ray\":\n","      return 11\n","    elif row_label == \"Green sea turtle\":\n","      return 12\n","    elif row_label == \"Hawksbill sea turtle\":\n","      return 13\n","    elif row_label == \"Kemp's ridley sea turtle\":\n","      return 14\n","    elif row_label == \"Leatherback sea turtle\":\n","      return 15\n","    elif row_label == \"Olive ridley sea turtle\":\n","      return 16\n","    else:\n","        None\n","\n","\n","def split(df, group):\n","    data = namedtuple('data', ['filename', 'object'])\n","    gb = df.groupby(group)\n","    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n","\n","\n","def create_tf_example(group, path):\n","    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n","        encoded_jpg = fid.read()\n","    encoded_jpg_io = io.BytesIO(encoded_jpg)\n","    image = Image.open(encoded_jpg_io)\n","    width, height = image.size\n","\n","    filename = group.filename.encode('utf8')\n","    image_format = b'jpg'\n","    # check if the image format is matching with your images.\n","    xmins = []\n","    xmaxs = []\n","    ymins = []\n","    ymaxs = []\n","    classes_text = []\n","    classes = []\n","\n","    for index, row in group.object.iterrows():\n","        xmins.append(row['xmin'] / width)\n","        xmaxs.append(row['xmax'] / width)\n","        ymins.append(row['ymin'] / height)\n","        ymaxs.append(row['ymax'] / height)\n","        classes_text.append(row['class'].encode('utf8'))\n","        classes.append(class_text_to_int(row['class']))\n","\n","    tf_example = tf.train.Example(features=tf.train.Features(feature={\n","        'image/height': dataset_util.int64_feature(height),\n","        'image/width': dataset_util.int64_feature(width),\n","        'image/filename': dataset_util.bytes_feature(filename),\n","        'image/source_id': dataset_util.bytes_feature(filename),\n","        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n","        'image/format': dataset_util.bytes_feature(image_format),\n","        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n","        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n","        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n","        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n","        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n","        'image/object/class/label': dataset_util.int64_list_feature(classes),\n","    }))\n","    return tf_example\n","\n","# We modify with the location of our files. Remember to take the time\n","# to check that the URL is the same as the Google Colab where you uploaded the\n","# file manually\n","output_path = \"test.record\"\n","image_dir = \"/content/dataset_original/Aquatic_Animals\"\n","csv_input = \"/content/test_labels.csv\"\n","\n","writer = tf.io.TFRecordWriter(output_path)\n","path = os.path.join(image_dir)\n","examples = pd.read_csv(csv_input)\n","grouped = split(examples, 'filename')\n","for group in grouped:\n","    tf_example = create_tf_example(group, path)\n","    writer.write(tf_example.SerializeToString())\n","\n","writer.close()\n","output_path = os.path.join(os.getcwd(), output_path)\n","print('Successfully created the TFRecords: {}'.format(output_path))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPYnCKqNHl5Bv8F2RqG6+7C","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
